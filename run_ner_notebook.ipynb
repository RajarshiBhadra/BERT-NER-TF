{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ee2ae2b",
   "metadata": {},
   "source": [
    "# Call Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1374b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "from model import BertNer\n",
    "from optimization import AdamWeightDecay, WarmUp\n",
    "from tokenization import FullTokenizer\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3108fde3",
   "metadata": {},
   "source": [
    "# Utility Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cc1750",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id, valid_ids=None, label_mask=None):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.valid_ids = valid_ids\n",
    "        self.label_mask = label_mask\n",
    "\n",
    "\n",
    "def readfile(filename):\n",
    "    '''\n",
    "    read file\n",
    "    '''\n",
    "    f = open(filename)\n",
    "    data = []\n",
    "    sentence = []\n",
    "    label = []\n",
    "    for line in f:\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == \"\\n\":\n",
    "            if len(sentence) > 0:\n",
    "                data.append((sentence, label))\n",
    "                sentence = []\n",
    "                label = []\n",
    "            continue\n",
    "        splits = line.split(' ')\n",
    "        sentence.append(splits[0])\n",
    "        label.append(splits[-1][:-1])\n",
    "\n",
    "    if len(sentence) > 0:\n",
    "        data.append((sentence, label))\n",
    "        sentence = []\n",
    "        label = []\n",
    "    return data\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        return readfile(input_file)\n",
    "\n",
    "\n",
    "class NerProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the CoNLL-2003 data set.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.txt\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"valid.txt\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"test.txt\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [\"O\", \"B-MISC\", \"I-MISC\",  \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"[CLS]\", \"[SEP]\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        examples = []\n",
    "        for i, (sentence, label) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = ' '.join(sentence)\n",
    "            text_b = None\n",
    "            label = label\n",
    "            examples.append(InputExample(\n",
    "                guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list, 1)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        textlist = example.text_a.split(' ')\n",
    "        labellist = example.label\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        valid = []\n",
    "        label_mask = []\n",
    "        start_position = 1\n",
    "        for i, word in enumerate(textlist):\n",
    "            token = tokenizer.tokenize(word)\n",
    "            tokens.extend(token)\n",
    "            label_1 = labellist[i]\n",
    "            labels.append(label_1)\n",
    "            valid.append(start_position)\n",
    "            start_position += len(token)\n",
    "            label_mask.append(True)\n",
    "        if len(tokens) >= max_seq_length - 1:\n",
    "            tokens = tokens[0:(max_seq_length - 2)]\n",
    "            labels = labels[0:(max_seq_length - 2)]\n",
    "            valid = valid[0:(max_seq_length - 2)]\n",
    "            label_mask = label_mask[0:(max_seq_length - 2)]\n",
    "        ntokens = []\n",
    "        segment_ids = []\n",
    "        label_ids = []\n",
    "        ntokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        valid.insert(0, 0)\n",
    "        label_mask.insert(0, True)\n",
    "        label_ids.append(label_map[\"[CLS]\"])\n",
    "        for i, token in enumerate(tokens):\n",
    "            ntokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "            if len(labels) > i:\n",
    "                label_ids.append(label_map[labels[i]])\n",
    "        ntokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "        valid.append(valid[-1]+1)\n",
    "        label_mask.append(True)\n",
    "        label_ids.append(label_map[\"[SEP]\"])\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(ntokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        label_mask = [True] * len(label_ids)\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "            label_ids.append(0)\n",
    "            label_mask.append(False)\n",
    "        while len(label_ids) < max_seq_length:\n",
    "            label_ids.append(0)\n",
    "            label_mask.append(False)\n",
    "        while len(valid) < max_seq_length:\n",
    "            valid.append(0)\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "        assert len(valid) == max_seq_length\n",
    "        assert len(label_mask) == max_seq_length\n",
    "\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" %\n",
    "                        \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" %\n",
    "                        \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(input_ids=input_ids,\n",
    "                          input_mask=input_mask,\n",
    "                          segment_ids=segment_ids,\n",
    "                          label_id=label_ids,\n",
    "                          valid_ids=valid,\n",
    "                          label_mask=label_mask))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60517061",
   "metadata": {},
   "source": [
    "# Input Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cad6573",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir    = '/content/BERT-NER-TF/data/'\n",
    "bert_model  = '/content/drive/MyDrive/bert-pretrained/bert-base-cased'\n",
    "output_dir  = 'out_base'\n",
    "max_seq_length = 128\n",
    "do_train = True\n",
    "do_eval  True\n",
    "eval_on = \"dev\"\n",
    "do_lower_case = True\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 64\n",
    "learning_rate = 5e-5\n",
    "num_train_epochs = 3\n",
    "warmup_proportion = .1\n",
    "weight_decay = .01\n",
    "adam_epsilon =1e-8 \n",
    "seed = 42\n",
    "multi_gpu = True\n",
    "gpus = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e042eaa7",
   "metadata": {},
   "source": [
    "# Parse Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0cbffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = NerProcessor()\n",
    "label_list = processor.get_labels()\n",
    "num_labels = len(label_list) + 1\n",
    "\n",
    "if os.path.exists(output_dir) and os.listdir(output_dir) and do_train:\n",
    "    raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(output_dir))\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "if do_train:\n",
    "    tokenizer = FullTokenizer(os.path.join(bert_model, \"vocab.txt\"), do_lower_case)\n",
    "\n",
    "if multi_gpu:\n",
    "    if len(gpus.split(',')) == 1:\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "    else:\n",
    "        gpus = [f\"/gpu:{gpu}\" for gpu in gpus.split(',')]\n",
    "        strategy = tf.distribute.MirroredStrategy(devices=gpus)\n",
    "else:\n",
    "    gpu = gpus.split(',')[0]\n",
    "    strategy = tf.distribute.OneDeviceStrategy(device=f\"/gpu:{gpu}\")\n",
    "\n",
    "train_examples = None\n",
    "optimizer = None\n",
    "num_train_optimization_steps = 0\n",
    "ner = None\n",
    "if do_train:\n",
    "    train_examples = processor.get_train_examples(data_dir)\n",
    "    num_train_optimization_steps = int(\n",
    "        len(train_examples) / train_batch_size) * num_train_epochs\n",
    "    warmup_steps = int(warmup_proportion *\n",
    "                       num_train_optimization_steps)\n",
    "    learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate=learning_rate,\n",
    "                                            decay_steps=num_train_optimization_steps,end_learning_rate=0.0)\n",
    "    if warmup_steps:\n",
    "        learning_rate_fn = WarmUp(initial_learning_rate=learning_rate,\n",
    "                                decay_schedule_fn=learning_rate_fn,\n",
    "                                warmup_steps=warmup_steps)\n",
    "    optimizer = AdamWeightDecay(\n",
    "        learning_rate=learning_rate_fn,\n",
    "        weight_decay_rate=weight_decay,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=adam_epsilon,\n",
    "        exclude_from_weight_decay=['layer_norm', 'bias'])\n",
    "\n",
    "    with strategy.scope():\n",
    "        ner = BertNer(bert_model, tf.float32, num_labels, max_seq_length)\n",
    "        loss_fct = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "label_map = {i: label for i, label in enumerate(label_list, 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4807ff14",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4056c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_train:\n",
    "    train_features = convert_examples_to_features(\n",
    "        train_examples, label_list, max_seq_length, tokenizer)\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "    logger.info(\"  Batch size = %d\", train_batch_size)\n",
    "    logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
    "\n",
    "    all_input_ids = tf.data.Dataset.from_tensor_slices(\n",
    "        np.asarray([f.input_ids for f in train_features],dtype=np.int32))\n",
    "    all_input_mask = tf.data.Dataset.from_tensor_slices(\n",
    "        np.asarray([f.input_mask for f in train_features],dtype=np.int32))\n",
    "    all_segment_ids = tf.data.Dataset.from_tensor_slices(\n",
    "        np.asarray([f.segment_ids for f in train_features],dtype=np.int32))\n",
    "    all_valid_ids = tf.data.Dataset.from_tensor_slices(\n",
    "        np.asarray([f.valid_ids for f in train_features],dtype=np.int32))\n",
    "    all_label_mask = tf.data.Dataset.from_tensor_slices(\n",
    "        np.asarray([f.label_mask for f in train_features]))\n",
    "\n",
    "    all_label_ids = tf.data.Dataset.from_tensor_slices(\n",
    "        np.asarray([f.label_id for f in eval_features],dtype=np.int32))\n",
    "\n",
    "    # Dataset using tf.data\n",
    "    train_data = tf.data.Dataset.zip(\n",
    "        (all_input_ids, all_input_mask, all_segment_ids, all_valid_ids, all_label_ids,all_label_mask))\n",
    "    shuffled_train_data = train_data.shuffle(buffer_size=int(len(train_features) * 0.1),\n",
    "                                            seed = seed,\n",
    "                                            reshuffle_each_iteration=True)\n",
    "    batched_train_data = shuffled_train_data.batch(train_batch_size)\n",
    "    # Distributed dataset\n",
    "    dist_dataset = strategy.experimental_distribute_dataset(batched_train_data)\n",
    "\n",
    "    loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "    epoch_bar = master_bar(range(num_train_epochs))\n",
    "    pb_max_len = math.ceil(\n",
    "        float(len(train_features))/float(train_batch_size))\n",
    "\n",
    "    def train_step(input_ids, input_mask, segment_ids, valid_ids, label_ids,label_mask):\n",
    "        def step_fn(input_ids, input_mask, segment_ids, valid_ids, label_ids,label_mask):\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = ner(input_ids, input_mask,segment_ids, valid_ids, training=True)\n",
    "                label_mask = tf.reshape(label_mask,(-1,))\n",
    "                logits = tf.reshape(logits,(-1,num_labels))\n",
    "                logits_masked = tf.boolean_mask(logits,label_mask)\n",
    "                label_ids = tf.reshape(label_ids,(-1,))\n",
    "                label_ids_masked = tf.boolean_mask(label_ids,label_mask)\n",
    "                cross_entropy = loss_fct(label_ids_masked, logits_masked)\n",
    "                loss = tf.reduce_sum(cross_entropy) * (1.0 / train_batch_size)\n",
    "            grads = tape.gradient(loss, ner.trainable_variables)\n",
    "            optimizer.apply_gradients(list(zip(grads, ner.trainable_variables)))\n",
    "            return cross_entropy\n",
    "\n",
    "        per_example_losses = strategy.experimental_run_v2(step_fn,\n",
    "                                 args=(input_ids, input_mask, segment_ids, valid_ids, label_ids,label_mask))\n",
    "        mean_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_example_losses, axis=0)\n",
    "        return mean_loss\n",
    "\n",
    "    for epoch in epoch_bar:\n",
    "        with strategy.scope():\n",
    "            for (input_ids, input_mask, segment_ids, valid_ids, label_ids,label_mask) in progress_bar(dist_dataset, total=pb_max_len, parent=epoch_bar):\n",
    "                loss = train_step(input_ids, input_mask, segment_ids, valid_ids, label_ids,label_mask)\n",
    "                loss_metric(loss)\n",
    "                epoch_bar.child.comment = f'loss : {loss_metric.result()}'\n",
    "        loss_metric.reset_states()\n",
    "\n",
    "    # model weight save\n",
    "    ner.save_weights(os.path.join(output_dir,\"model.h5\"))\n",
    "    # copy vocab to output_dir\n",
    "    shutil.copyfile(os.path.join(bert_model,\"vocab.txt\"),os.path.join(output_dir,\"vocab.txt\"))\n",
    "    # copy bert config to output_dir\n",
    "    shutil.copyfile(os.path.join(bert_model,\"bert_config.json\"),os.path.join(output_dir,\"bert_config.json\"))\n",
    "    # save label_map and max_seq_length of trained model\n",
    "    model_config = {\"bert_model\":bert_model,\"do_lower\":do_lower_case,\n",
    "                    \"max_seq_length\":max_seq_length,\"num_labels\":num_labels,\n",
    "                    \"label_map\":label_map}\n",
    "    json.dump(model_config,open(os.path.join(output_dir,\"model_config.json\"),\"w\"),indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a013d9d",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039874ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_eval:\n",
    "    # load tokenizer\n",
    "    tokenizer = FullTokenizer(os.path.join(output_dir, \"vocab.txt\"), do_lower_case)\n",
    "    # model build hack : fix\n",
    "    config = json.load(open(os.path.join(output_dir,\"bert_config.json\")))\n",
    "    ner = BertNer(config, tf.float32, num_labels, max_seq_length)\n",
    "    ids = tf.ones((1,128),dtype=tf.int32)\n",
    "    _ = ner(ids,ids,ids,ids, training=False)\n",
    "    ner.load_weights(os.path.join(output_dir,\"model.h5\"))\n",
    "\n",
    "    # load test or development set based on argsK\n",
    "    if eval_on == \"dev\":\n",
    "        eval_examples = processor.get_dev_examples(data_dir)\n",
    "    elif eval_on == \"test\":\n",
    "        eval_examples = processor.get_test_examples(data_dir)\n",
    "\n",
    "    eval_features = convert_examples_to_features(\n",
    "        eval_examples, label_list, max_seq_length, tokenizer)\n",
    "    logger.info(\"***** Running evalution *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "\n",
    "    all_input_ids = tf.data.Dataset.from_tensor_slices(\n",
    "        np.asarray([f.input_ids for f in eval_features],dtype=np.int32))\n",
    "    all_input_mask = tf.data.Dataset.from_tensor_slices(\n",
    "        np.asarray([f.input_mask for f in eval_features],dtype=np.int32))\n",
    "    all_segment_ids = tf.data.Dataset.from_tensor_slices(\n",
    "        np.asarray([f.segment_ids for f in eval_features],dtype=np.int32))\n",
    "    all_valid_ids = tf.data.Dataset.from_tensor_slices(\n",
    "        np.asarray([f.valid_ids for f in eval_features],dtype=np.int32))\n",
    "\n",
    "    all_label_ids = tf.data.Dataset.from_tensor_slices(\n",
    "        np.asarray([f.label_id for f in eval_features],dtype=np.int32))\n",
    "\n",
    "    eval_data = tf.data.Dataset.zip(\n",
    "        (all_input_ids, all_input_mask, all_segment_ids, all_valid_ids, all_label_ids))\n",
    "    batched_eval_data = eval_data.batch(eval_batch_size)\n",
    "\n",
    "    loss_metric = tf.keras.metrics.Mean()\n",
    "    epoch_bar = master_bar(range(1))\n",
    "    pb_max_len = math.ceil(\n",
    "        float(len(eval_features))/float(eval_batch_size))\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    label_map = {i : label for i, label in enumerate(label_list,1)}\n",
    "    for epoch in epoch_bar:\n",
    "        for (input_ids, input_mask, segment_ids, valid_ids, label_ids) in progress_bar(batched_eval_data, total=pb_max_len, parent=epoch_bar):\n",
    "                logits = ner(input_ids, input_mask,\n",
    "                             segment_ids, valid_ids, training=False)\n",
    "                logits = tf.argmax(logits,axis=2)\n",
    "                for i, label in enumerate(label_ids):\n",
    "                    temp_1 = []\n",
    "                    temp_2 = []\n",
    "                    for j,m in enumerate(label):\n",
    "                        if j == 0:\n",
    "                            continue\n",
    "                        elif label_ids[i][j].numpy() == len(label_map):\n",
    "                            y_true.append(temp_1)\n",
    "                            y_pred.append(temp_2)\n",
    "                            break\n",
    "                        else:\n",
    "                            temp_1.append(label_map[label_ids[i][j].numpy()])\n",
    "                            temp_2.append(label_map[logits[i][j].numpy()])\n",
    "    report = classification_report(y_true, y_pred,digits=4)\n",
    "    output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "        logger.info(\"\\n%s\", report)\n",
    "        writer.write(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
